# Notes for "Introduction to Artificial Intelligence for Security Professionals", by the Cylance Data Science Team

## Introduction

The field of AI encompasses three areas of research
- Artificial Superintelligence (ASI)
 * The goal is to produce computer with "perfect memory and unlimited
   analytical power"
- Artificial General Intelligence (AGI)
 * Refers to machine as intelligent as a human, i.e., it is able to pass
   the "Turing Test"
 * Most experts agree we are at least decades away from AGI
- Artificial Narrow Intelligence (ANI)
 * Exploits computer's superior ability to solve domain-specific
   problems with vast data difficult for impossible for a human to
   handle
 * This book focuses on a sub-area of this area, machine learning

Code of this book is available at:
    https://github.com/cylance/IntroductionToMachineLearningForSecurityPros

## Chapter One: Clustering

Clustering refers to a variety of techniques for sub-dividing samples
into distinct sub-groups, or clusters, based on similarity among their
key features and attributes. 

Techniques discussed: K-mean and DBSCAN

Typical stages of a clustering procedure:
- Data selection and sampling
- Feature extraction
- Feature encoding and vectorization
- Model computation and graphing
- Model validation and testing


### K-mean
- Hyperparameter: K
- Easy and works many times

Limitations:
- With randomly chosen initial centroids, the algorithm may not converge
- Defining "K" appropriately requires experience, patience and luck

### DBSCAN: Density-based Spatial Clustering of an Application with Noise

It constructs clusters in regions where vectors are most densely packed
and considers points in sparser regions to be noise. It
- discovers the number of clusters to create
- can construct clusters of any shape and size

DBSCAN hyperparameters: Epsilon (Eps) and Minimum Points (MinPts)

- Eps: radius of the point's neighborhood
- MinPts: minimum number of points that must appear within an Eps
  neighborhood for the point inside to be included in cluster

DBSCAN assigns each point to one of three types:
- core point
- border point
- noise point

How to tune DBSCAN:
- Method 1: tune Eps and MinPts values
- Method 2: redefine Eps hyperparameter function by using different
  distance metrics: Euclidean, Manhattan or City Block, Cosine
  similarity

Limitations:
- Extremely sensitive to even small changes in MinPts and Eps settings
- High computational cost in high dimensional cases
- Poor performance w.r.t. datasets that result in regions of varying
  densities

### Silhouette score
The silhouette score (or silhouette value) is a measure of how similar
an object is to its own cluster compared to other clusters. The silhouette
score ranges between -1 and 1; the higher the score, the better matched 
the object is to its cluster.

## Chapter Two: Classification

Classification refers to a set of computational methods for predicting
the likelihood that a given sample belongs to a predefined class.

Techniques discussed: Logistic regression and CART decision tree
algorithm

Typical phases of a supervised learning procedure:
- Model training: application of classifier to training data
- Validation: use of validation data to assess its accuracy and optimize
  hyperparameters; similar to testing; needs enough data available;
  omitted from the chapter's discussion 
- Testing: assessment of the model's accuracy with test data that was
  withheld
- Deployment: application of model to predict the class membership of
  new, unlabeled data

This chapter only considers binary classification problems, i.e., 
how to assign an object to one of two classes.

The algorithms used to perform classification are referred to as
"classifiers".

Classification is an example of *supervised* learning: model is built
using known labeled samples with certain properties.
 
Clustering is an example of *unsupervised* learning: properties that
distinguish one group from another must be discovered.

A large quantity of data is needed to produce an accurate model. This
data is typically divided into two or three distinct sets for training,
validation, and testing. The larger the training set, the more likely
the classifier is to produce an accurate model. However, enough testing
data must be retained to conduct a reliable assessment of the model's
accuracy.

### Logistic Regression (LR)
A linear classifier that utilizes straight lines and planes to
distinguish vectors belonging to one class from another

Like in the case of normalization in clustering, penalty parameters are
used in LR algorithm to "regularize" regression weight values.

Terms:
- TP: True Positive
- TN: True Negative
- FP: False Positive
- FN: False Negative
- |S|: Cardinality of set S

Validation metrics:
- Precision: |TP|/(|TP| + |FP|)
- Recall (True positive rate): |TP| / (|TP| + |FN|)
- Mean accuracy: (|TP| + |TN|)/(Total number of samples), where "Total
  number of samples" is (|TP| + |TN| + |FP| + |FN|)
- Misclassification rate: 1 - "Mean accuracy"
- False positive rate: |FP| / (Number of actual nagtives), where "Number
  of actual negatives" is |TN| + |FP|
- Specificity: 1 - "False positive rate"
- Prevalence: (Number of actual positives) / (Total number of samples)

Receiver Operating Characteristic (ROC) curves:
- ROC curves can be used to visualize the performance of a binary
  classification algorithm, by plotting the TPR (y-axis) against the FPR
  (x-axis) at each probability threshold.

LR pitfalls and limitations:
- The underlying data must intrinsically support linear classification
- LR is vulnerable to under-fitting when datasets have many outliers,
  features with disproportionately large values, and sets of features
  that are hightly correlated

### Decision Trees (DT)
DT algorithms determine whether a given vector belongs to one class or
another by defining a sequance of "if-then-else" decision rules.

For categorical class labels, we would utilize "DT classification"
algorithm

For continuous class labels, we would use "DT regression" algorithm

LR classifiers carve feature space using straight lines and planes, and
are more prone to under-fitting.
DTs carve feature space using rectangles, and are susceptable to
over-fitting.

## Chapter Three: Probability

Probability is considered as a predicative modeling technique for
classifying and clustering sample.

Techniques discussed: Naive Bayes (NB) and Gaussian Mixture Model (GMM)

*Skip this chapter for now*

## Chapter Four: Deep Learning

Deep learning refers to a wide range of learning methods primarily based
on the use of neural networks.

Techniques discussed: Long Short-Term Memory (LSTM) and Convolutional
Neural Network (CNN) type of neural networks.

It the "multi-layered" approach, employing hidden layers, that
distinguishes deep learning from all other machine learning methods.

### Layers in neural networks
- Input layer: the nodes in input layer are passive
- Hidden layer(s): composed of nodes that perform the heavy lifting of
  the deep learning process
  * Receiving feature values from input layer
  * Applying weights
  * Summing the products
  * Applying the activation function
  * Outputing the result
- Output layer: for classification, the output layer will incorporate
  a node for every possible class assignment

Generally speaking, the more hidden layers we use, the more overall
capacity the neural network has to solve complex, data-intensive
problems. However, neural networks with excess capacity may product
models that over-fit the data; those with insufficient capacity may
product models with unacceptable error rates


### The Long Short-Term Memory (LSTM) Neural Network
LSTM is a type of recurrent neural network (RNN). The RNN is the more
appropriate choice for time-series problems.

Feedforward networks have no notion of time.

RNNs are distinguished from the feedforward type by their ability to
consider not only the current input, but also its relationship to the
input that immediately preceded it. To accomplish this, RNNs employ
feedback loops.

In the LSTM network, nodes in each hidden layer are replaced by memory
blocks, each of which can contain one or more memory cells. Memory cells
are not equipped with activation functions; instead they utilize three
types of gates: forget gate, input gate, output gate.

Gates enable LSTMs to retain and reuse relevant information spread
across very long sequences of time-series data.

### The Convolutional Neural Network (CNN)
CNNs does not utilize a fully-connected architecture. Instead, each
convolution layer connnects its nodes only to contiguous sets of nodes
in the previous layer. The connections are defined using the CNN
hyperparameters 'size' and 'stride'.

The wight tying technique causes all nodes to apply the same weight
values when performing their computations. This drastically reduces the
number of parameters a CNN model must compute.

